{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implementation of RNN\n",
    "forked from https://github.com/pangolulu/rnn-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import getSentenceData\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "    def backward(self, x, diff):\n",
    "        output = np.tanh(x)\n",
    "        return (1.0 - np.square(output)) * diff\n",
    "    \n",
    "class sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1.0/(1.0+np.exp(-x))\n",
    "    def backward(self, x, diff):\n",
    "        output = self.forward(x)\n",
    "        return (1.0-output)*output*diff\n",
    "    \n",
    "class softmax:\n",
    "    def predict(self, x):\n",
    "        return np.exp(x)/np.sum(np.exp(x))\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        return -np.log(probs[0,y])\n",
    "    \n",
    "    def diff(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        probs[:,y] -= 1.0\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gate for calculating derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiplyGate:\n",
    "    def forward(self, x, w):\n",
    "        return np.dot(x, w)\n",
    "    \n",
    "    def backward(self, x, w, dz):\n",
    "        dw = np.dot(x.T, dz)\n",
    "        dx = np.dot(dz, w.T)\n",
    "        return dw, dx\n",
    "    \n",
    "class AddGate:\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2\n",
    "    \n",
    "    def backward(self, x1, x2, dz):\n",
    "        dx1 = dz\n",
    "        dx2 = dz\n",
    "        return dx1, dx2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One layer of RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mulGate = MultiplyGate()\n",
    "addGate = AddGate()\n",
    "activation = Tanh()\n",
    "\n",
    "class RNNLayer:\n",
    "    def forward(self, x, prev_s, U, W, V):\n",
    "        self.mulu = mulGate.forward(x, U)\n",
    "        self.mulw = mulGate.forward(prev_s, W)\n",
    "        self.adduw = addGate.forward(self.mulu, self.mulw)\n",
    "        self.state = activation.forward(self.adduw)\n",
    "        self.mulv = mulGate.forward(self.state, V)\n",
    "    \n",
    "    def backward(self, x, prev_s, U, W, V, dmulv):       \n",
    "        self.forward(x, prev_s, U, W, V)\n",
    "        dV, dVx = mulGate.backward(self.state, V, dmulv)\n",
    "        dadd = activation.backward(self.adduw, dVx)\n",
    "        dmulu, dmulw = addGate.backward(self.mulu, self.mulw, dadd)\n",
    "        dU, dUx = mulGate.backward(x, U, dmulu)\n",
    "        dW, dWx = mulGate.backward(prev_s, W, dmulw)\n",
    "        return dU, dW, dV\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = softmax()\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_dim, hidden_nodes, output_dim, lr = 0.001, bptt_truncate = 4):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_dim = output_dim\n",
    "        self.U = np.random.random([input_dim, hidden_nodes])*0.01\n",
    "        self.W = np.random.random([hidden_nodes, hidden_nodes])*0.01\n",
    "        self.V = np.random.random([hidden_nodes, output_dim])*0.01\n",
    "        self.lr = lr\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "\n",
    "    def forward(self, x):  \n",
    "        # total number of time steps\n",
    "        # each steps input a word, a word is a vector of length 8000 \n",
    "        self.time_steps = len(x)\n",
    "        layers = []\n",
    "        prev_s = np.zeros([1, self.hidden_nodes])\n",
    "        for t in range(self.time_steps):\n",
    "            layer = RNNLayer()\n",
    "            input_vec = np.zeros([1, self.input_dim])\n",
    "            input_vec[0,x[t]] = 1\n",
    "            layer.forward(input_vec, prev_s, self.U, self.W, self.V)\n",
    "            prev_s = layer.state\n",
    "            layers.append(layer)\n",
    "        return layers\n",
    "    \n",
    "    def backward(self, x, y):\n",
    "        dU = np.zeros_like(self.U)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        dV = np.zeros_like(self.V)\n",
    "        layers = self.forward(x)\n",
    "        for t in range(self.time_steps):\n",
    "            dmulv = output.diff(layers[t].mulv, y[t])\n",
    "            input_vec = np.zeros([1,self.input_dim])\n",
    "            prev_s = np.zeros([1,self.hidden_nodes])\n",
    "            input_vec[0,x[t]] = 1\n",
    "            dU_t, dW_t, dV_t = layers[t].backward(input_vec, prev_s, self.U, self.W, self.V, dmulv)\n",
    "            for i in range(t-1,max(-1, t-self.bptt_truncate-1),-1):\n",
    "                input_vec = np.zeros([1,self.input_dim])\n",
    "                input_vec[0,x[i]] = 1\n",
    "                prev_s_i = np.zeros([1,self.hidden_nodes]) if i == 0 else layers[i-1].state\n",
    "                dU_i, dW_i, dV_i = layers[i].backward(input_vec, prev_s_i, self.U, self.W, self.V, dmulv)\n",
    "                dU_t += dU_i\n",
    "                dW_t += dW_i\n",
    "                dV_t += dV_i\n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "            dV += dV_t\n",
    "        return dU, dW, dV\n",
    "    \n",
    "    def sgd_optimizer(self, x, y, lr):\n",
    "        dU, dW, dV = self.backward(x,y)\n",
    "        self.U -= lr*dU\n",
    "        self.W -= lr*dW\n",
    "        self.V -= lr*dV\n",
    "    \n",
    "    def caculate_loss(self, x, y):\n",
    "        loss = 0.0\n",
    "        for example in range(len(y)):\n",
    "            single_loss = 0.0\n",
    "            layers = self.forward(x[example])\n",
    "            for j,layer in enumerate(layers):                \n",
    "                single_loss += output.loss(layer.mulv, y[example][j])\n",
    "            loss += (single_loss/len(layers))\n",
    "        return loss/len(y)\n",
    "                \n",
    "    def train(self, x, y, lr=0.005, nepoch=100, evaluate_loss_after=5):     \n",
    "        for epoch in range(nepoch):\n",
    "            if epoch % evaluate_loss_after == 0:\n",
    "                loss = self.caculate_loss(x,y)\n",
    "                print(\"Epoch=%d   Loss=%f\" % (epoch, loss))\n",
    "            for i in range(len(y)):\n",
    "                self.sgd_optimizer(x[i], y[i], lr) # x[i], y[i] is a list\n",
    "                \n",
    "    def predict(self, x):\n",
    "        output = softmax()\n",
    "        layers = self.forward(x)\n",
    "        predict_y = [np.argmax(output.predict(layer.mulv)) for layer in layers]\n",
    "        return predict_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example\n",
    "\n",
    "train: input a sentence for each RNN layer, no matter how long it is, each RNN layer outputs one word's next word\n",
    "\n",
    "test: input a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dim = 8000\n",
    "hidden_dim = 100\n",
    "output_dim = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79171 sentences.\n",
      "Found 65720 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'bethesda' and appeared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n",
      "\n",
      "X_train shape: (78483,)\n",
      "y_train shape: (78483,)\n",
      "x:\n",
      "SENTENCE_START what are n't you understanding about this ? !\n",
      "[1, 51, 27, 16, 10, 853, 53, 25, 34, 69]\n",
      "\n",
      "y:\n",
      "what are n't you understanding about this ? ! SENTENCE_END\n",
      "[51, 27, 16, 10, 853, 53, 25, 34, 69, 0]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, index_to_word = getSentenceData('data/reddit-comments-2015-08.csv', input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0   Loss=8.987161\n",
      "Epoch=1   Loss=8.985943\n",
      "Epoch=2   Loss=8.984481\n",
      "Epoch=3   Loss=8.982446\n",
      "Epoch=4   Loss=8.979229\n",
      "Epoch=5   Loss=8.973298\n",
      "Epoch=6   Loss=8.958033\n",
      "Epoch=7   Loss=8.018889\n",
      "Epoch=8   Loss=7.010689\n",
      "Epoch=9   Loss=6.628749\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "losses = model.train(X_train[:100], y_train[:100], lr=0.001, nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0   Loss=6.421941\n",
      "Epoch=1   Loss=6.282418\n",
      "Epoch=2   Loss=6.179307\n",
      "Epoch=3   Loss=6.098684\n",
      "Epoch=4   Loss=6.033502\n",
      "Epoch=5   Loss=5.979838\n",
      "Epoch=6   Loss=5.935124\n",
      "Epoch=7   Loss=5.897456\n",
      "Epoch=8   Loss=5.865345\n",
      "Epoch=9   Loss=5.837620\n"
     ]
    }
   ],
   "source": [
    "losses = model.train(X_train[:100], y_train[:100], lr=0.001, nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_y = model.predict(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
